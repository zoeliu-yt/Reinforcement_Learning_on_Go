{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL_Chess_Week_9_Iter.ipynb","provenance":[{"file_id":"12u_mvyDU9TRq_NIs3j9esUoPOmZvrWH3","timestamp":1610780795696}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNe+97zIjoiQd2Rr5kdrwHR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Uq9ziHnmKvkI"},"source":["There are two major intermidiate steps when solving chess problem:\n","1. Move Chess:\n","\n",">> Objective: find the shortest path between 2 spots on a chess board\n","\n",">> Motivation: Move Chess effectively\n","\n","2. Capture Chess\n","\n",">> Objective: capture as many pieces from the opponent within centain fullmoves\n","\n",">> Motivation: Piece captures as one way of reward.\n","\n","\n","Today we focus on Move chess"]},{"cell_type":"markdown","metadata":{"id":"aTDi57bymfrV"},"source":["# Load and Import Package"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VS1maVXrqT3D","executionInfo":{"status":"ok","timestamp":1611721222553,"user_tz":300,"elapsed":8218,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"ec05e466-4051-40a0-9e1c-804f7b473e82"},"source":["!pip install --upgrade git+https://github.com/arjangroen/RLC.git \n","!pip install python-chess  \n","# Python-Chess is the Python Chess Package that handles the chess environment"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/arjangroen/RLC.git\n","  Cloning https://github.com/arjangroen/RLC.git to /tmp/pip-req-build-g9iqp115\n","  Running command git clone -q https://github.com/arjangroen/RLC.git /tmp/pip-req-build-g9iqp115\n","Building wheels for collected packages: RLC\n","  Building wheel for RLC (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for RLC: filename=RLC-0.3-cp36-none-any.whl size=22566 sha256=1d0be4daf5c123f601567b83cf2c80ca800b1b298a491c3ab7d60308b08e1e2f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-putlw2_u/wheels/04/68/a5/cb835cd3d76a49de696a942739c71a56bfe66d0d8ea7b4b446\n","Successfully built RLC\n","Installing collected packages: RLC\n","Successfully installed RLC-0.3\n","Requirement already satisfied: python-chess in /usr/local/lib/python3.6/dist-packages (0.23.11)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UiRPTTKQBu6R"},"source":["from RLC.move_chess.environment import Board\n","from RLC.move_chess.agent import Piece\n","from RLC.move_chess.learn import Reinforce\n","import inspect\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JqVqulX-WJQT"},"source":["# Load Chess"]},{"cell_type":"code","metadata":{"id":"KZHb_HqS081K"},"source":["p = Piece(piece='king') # select a chess agent (knight, bishop or rook)\n","env = Board() # 8*8 chess board\n","r = Reinforce(p,env) \n","#env.render()\n","# env.visual_board # S start position, F is terminate position"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yy7GEyNWYHWB","executionInfo":{"status":"ok","timestamp":1610954231691,"user_tz":480,"elapsed":450,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"dd7d0065-95e0-49db-a940-02fa1e590d39"},"source":["r.visualize_policy()\n","#r.agent.policy"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', '↑', '↑', '↑'],\n"," ['↑', '↑', '↑', '↑', '↑', 'F', '↑', '↑']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N9SRO2ctZrmx","executionInfo":{"status":"ok","timestamp":1610954233170,"user_tz":480,"elapsed":399,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"0685174c-21fb-49a2-b1e4-a7ef73c7d946"},"source":["r.agent.value_function"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0., 0., 0., 0.]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"XxBNCqy9ZnEJ"},"source":["# 1.1 State Evaluation\n","\n","If we want our agent to optimize its rewards, we want its policy to guide behavior towards the states with the highest value. This value can be estimated using bootstrapping:\n","\n","A state (s) is as valuable (V) as the successor state (s') plus the reward (R) for going from s to s'.\n","Since there can be mulitple actions (a) and multiple successor states they are summed and weighted by their probability (pi).\n","In a non-deterministic environment, a given action could result in multiple successor states. We don't have to take this into account for this problem because move chess is a deterministic game.\n","Successor state values are discounted with discount factor (gamma) that varies between 0 and 1."]},{"cell_type":"code","metadata":{"id":"sQTpOjgNaoIq"},"source":["def evaluate_state(self, state, gamma=0.9, synchronous=True):\n","        \"\"\"\n","        Calculates the value of a state based on the successor states and the immediate rewards.\n","        Args:\n","            state: tuple of 2 integers 0-7 representing the state\n","            gamma: float, discount factor\n","            synchronous: Boolean\n","        Returns: The expected value of the state under the current policy.\n","        \"\"\"\n","        greedy_action_value = np.max(self.agent.policy[state[0], state[1], :]) # get max action value at give state\n","        greedy_indices = [i for i, a in enumerate(self.agent.policy[state[0], state[1], :]) if\n","                          a == greedy_action_value]  # list the index of all actions with max action value\n","        prob = 1 / len(greedy_indices)  # probability of an action occuring\n","        state_value = 0 # set V(S) = 0\n","        for i in greedy_indices: # for all actions with max action value\n","            self.env.state = state  # reset state to the one being evaluated\n","            reward, episode_end = self.env.step(self.agent.action_space[i]) # get reward\n","            if synchronous: # get successor state value\n","                successor_state_value = self.agent.value_function_prev[self.env.state] \n","                # if synchronous, successor state value is in the same iteration of policy evaluation\n","            else:\n","                # if not,  successor state value could be previous or the current value funtion,or combined\n","                successor_state_value = self.agent.value_function[self.env.state] # otherwise \n","            state_value += (prob * (\n","                    reward + gamma * successor_state_value))  # sum up rewards and discounted successor state value in equation(*)\n","        return state_value"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmBOhTXtYgsO","executionInfo":{"status":"ok","timestamp":1611723809986,"user_tz":300,"elapsed":337,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"b8da0ad3-be80-4a77-9dfa-253db649b806"},"source":["state = (0,0)\n","r.agent.value_function[0,0] = r.evaluate_state(state,gamma=1)\n","r.agent.value_function.astype(int)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0],\n","       [ 0,  0,  0,  0,  0,  0,  0,  0]])"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"id":"4oPgx_gvNA8G"},"source":["# 1.2 Policy Evaluation\n","\n","Policy evaluation is the act of doe state evaluation for each state in the statespace\n","As you can see in my implementatin I simply iterate over all state and update the value function\n","This is the algorithm provided by Sutton and Barto:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isaOMj94NDml","executionInfo":{"status":"ok","timestamp":1611723465175,"user_tz":300,"elapsed":557,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"ad9cfa38-6389-4459-f921-38546f0eb1dd"},"source":["print(inspect.getsource(r.evaluate_policy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["    def evaluate_policy(self, gamma=0.9, synchronous=True):\n","        self.agent.value_function_prev = self.agent.value_function.copy()  # For synchronous updates\n","        for row in range(self.agent.value_function.shape[0]):\n","            for col in range(self.agent.value_function.shape[1]):\n","                self.agent.value_function[row, col] = self.evaluate_state((row, col), gamma=gamma,\n","                                                                          synchronous=synchronous)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MDKaj5wZB-38"},"source":["def evaluate_policy(self, gamma=0.9, synchronous=True):\n","        self.agent.value_function_prev = self.agent.value_function.copy()  # For synchronous updates\n","        for row in range(self.agent.value_function.shape[0]): # in each row\n","            for col in range(self.agent.value_function.shape[1]): # in each column\n","                self.agent.value_function[row, col] = self.evaluate_state((row, col), gamma=gamma, # in each action, update value function\n","                                                                          synchronous=synchronous)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMPO6vokNGO2","executionInfo":{"status":"ok","timestamp":1611723819222,"user_tz":300,"elapsed":374,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"a34a3cb3-a8d4-464f-8eec-e3bd33bf4398"},"source":["r.evaluate_policy(gamma=1)\n","r.agent.value_function.astype(int)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1, -1, -1, -1],\n","       [-1, -1, -1, -1, -1,  0, -1, -1]])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"zAxOYPBPNW78"},"source":["We can iterate this until the value function is stable:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YmhdwhSlNWOA","executionInfo":{"status":"ok","timestamp":1611723827012,"user_tz":300,"elapsed":4132,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"505135e2-f51b-4b06-fb70-81de75de310c"},"source":["eps=0.1\n","k_max = 1000\n","value_delta_max = 0\n","gamma = 1\n","synchronous=True\n","#value_delta_max = 0\n","for k in range(k_max):\n","    r.evaluate_policy(gamma=gamma,synchronous=synchronous)\n","    value_delta = np.max(np.abs(r.agent.value_function_prev - r.agent.value_function))\n","    value_delta_max = value_delta\n","    if value_delta_max < eps:\n","        print('converged at iter',k)\n","        break"],"execution_count":null,"outputs":[{"output_type":"stream","text":["converged at iter 428\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXuuLsmzOhW7","executionInfo":{"status":"ok","timestamp":1611723840677,"user_tz":300,"elapsed":296,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"3dd1982b-2c19-40f2-c402-039881724c06"},"source":["r.agent.value_function.astype(int)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-185, -183, -181, -179, -177, -175, -174, -175],\n","       [-182, -181, -179, -177, -174, -172, -171, -171],\n","       [-179, -177, -175, -172, -168, -166, -165, -164],\n","       [-174, -172, -169, -164, -160, -156, -154, -154],\n","       [-169, -167, -161, -154, -147, -142, -139, -139],\n","       [-164, -161, -154, -142, -131, -122, -119, -120],\n","       [-161, -157, -147, -131, -106,  -93,  -92, -102],\n","       [-160, -156, -145, -126,  -93,    0,  -77,  -93]])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"ukn3K5XkLzt9"},"source":["# 1.3 Policy Improvement\n","\n","Now that we know what the values of the states are, we want to improve our Policy so that we the behavior is guided towards the state with the highest value. Policy Improvement is simply the act of making the policy greedy with respect to the value function.\n","\n","In my implementation, we do this by setting the value of the action that leads to the most valuable state to 1 (while the rest remains 0)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2QC2HE2L-1P","executionInfo":{"status":"ok","timestamp":1611723175019,"user_tz":300,"elapsed":352,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"9f424390-3c98-459c-9564-ae091b517bf9"},"source":["print(inspect.getsource(r.improve_policy))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["    def improve_policy(self):\n","        \"\"\"\n","        Finds the greedy policy w.r.t. the current value function\n","        \"\"\"\n","\n","        self.agent.policy_prev = self.agent.policy.copy()\n","        for row in range(self.agent.action_function.shape[0]):\n","            for col in range(self.agent.action_function.shape[1]):\n","                for action in range(self.agent.action_function.shape[2]):\n","                    self.env.state = (row, col)  # reset state to the one being evaluated\n","                    reward, episode_end = self.env.step(self.agent.action_space[action])\n","                    successor_state_value = 0 if episode_end else self.agent.value_function[self.env.state]\n","                    self.agent.policy[row, col, action] = reward + successor_state_value\n","\n","                max_policy_value = np.max(self.agent.policy[row, col, :])\n","                max_indices = [i for i, a in enumerate(self.agent.policy[row, col, :]) if a == max_policy_value]\n","                for idx in max_indices:\n","                    self.agent.policy[row, col, idx] = 1\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iDbeBdTH7AMi"},"source":["def improve_policy(self):\n","        \"\"\"\n","        Finds the greedy policy w.r.t. the current value function\n","        \"\"\"\n","\n","        self.agent.policy_prev = self.agent.policy.copy() # get pi(i)\n","        for row in range(self.agent.action_function.shape[0]): # for each row\n","            for col in range(self.agent.action_function.shape[1]): # for each column\n","                for action in range(self.agent.action_function.shape[2]): # for each action\n","                    self.env.state = (row, col)  # reset state to the one being evaluated\n","                    reward, episode_end = self.env.step(self.agent.action_space[action]) # make step based on action\n","                    successor_state_value = 0 if episode_end else self.agent.value_function[self.env.state] # update successor state value\n","                    self.agent.policy[row, col, action] = reward + successor_state_value # get pi(i+1) using (**)\n","\n","                max_policy_value = np.max(self.agent.policy[row, col, :]) # get max policy value at given state\n","                max_indices = [i for i, a in enumerate(self.agent.policy[row, col, :]) if a == max_policy_value] # get the index of policy with max policy value\n","                for idx in max_indices:\n","                    self.agent.policy[row, col, idx] = 1 # 1 if the policy value is max, otherwise 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXH3njNnMKC1","executionInfo":{"status":"ok","timestamp":1611723849203,"user_tz":300,"elapsed":342,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"fb0b0230-63e0-4570-bd09-009980ea108b"},"source":["r.improve_policy()\n","r.visualize_policy()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n"," ['↘', '↘', '↘', '↘', '↘', '↓', '↙', '↙'],\n"," ['→', '→', '→', '→', '→', 'F', '←', '←']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qn61G-WzOpe9"},"source":["# 1.4 Policy Iteration\n","\n","We can now find the optimal policy by doing policy evaluation and policy improvement untill the policy is stable"]},{"cell_type":"code","metadata":{"id":"XP68KzxVOwqP","executionInfo":{"status":"ok","timestamp":1613538803887,"user_tz":300,"elapsed":382,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}}},"source":["print(inspect.getsource(r.policy_iteration))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cOlI9MJp_4Yg"},"source":["def policy_iteration(self, eps=0.1, gamma=0.9, iteration=1, k=32, synchronous=True):\n","        \"\"\"\n","        Finds the optimal policy\n","        Args:\n","            eps: float, exploration rate\n","            gamma: float, discount factor\n","            iteration: the iteration number\n","            k: (int) maximum amount of policy evaluation iterations\n","            synchronous: (Boolean) whether to use synchronous are asynchronous back-ups \n","\n","        Returns:\n","\n","        \"\"\"\n","        policy_stable = True\n","        print(\"\\n\\n______iteration:\", iteration, \"______\")\n","        print(\"\\n policy:\")\n","        self.visualize_policy() \n","\n","        # Evaluate Policy\n","        print(\"\")\n","        value_delta_max = 0\n","        for _ in range(k): # max number of iteration for policy evaluation\n","            self.evaluate_policy(gamma=gamma, synchronous=synchronous) # update V(s)\n","            value_delta = np.max(np.abs(self.agent.value_function_prev - self.agent.value_function)) # get delta\n","            value_delta_max = value_delta\n","            if value_delta_max < eps: # terminate the loop if delta is small \n","                break\n","        print(\"Value function for this policy:\")\n","        print(self.agent.value_function.round().astype(int))\n","        action_function_prev = self.agent.action_function.copy()\n","\n","        # Improve Policy\n","        print(\"\\n Improving policy:\")\n","        self.improve_policy()\n","\n","        # Check if Stable\n","        policy_stable = self.agent.compare_policies() < 1 #check if policy function is similar with previous iteration\n","        print(\"policy diff:\", policy_stable)\n","\n","        if not policy_stable and iteration < 1000: # if policy is not stable, restart the whole function\n","            iteration += 1\n","            self.policy_iteration(iteration=iteration)\n","        elif policy_stable:\n","            print(\"Optimal policy found in\", iteration, \"steps of policy evaluation\")\n","        else:\n","            print(\"failed to converge.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lPdpNUIO0Nb","executionInfo":{"status":"ok","timestamp":1611723918056,"user_tz":300,"elapsed":287,"user":{"displayName":"Yuetong Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjled0guJ9UlVUNssv1mWyc5TCUXbCOP3JIGHMPzQ=s64","userId":"07904439503082268536"}},"outputId":"5934a7f9-5d30-4308-9c82-42164c63e2af"},"source":["r.policy_iteration()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","______iteration: 1 ______\n","\n"," policy:\n","[['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↓', '↙'],\n"," ['↘', '↘', '↘', '↘', '↘', '↓', '↙', '↙'],\n"," ['→', '→', '→', '→', '→', 'F', '←', '←']]\n","\n","Value function for this policy:\n","[[-5 -5 -5 -5 -5 -5 -5 -5]\n"," [-5 -5 -5 -5 -5 -5 -5 -5]\n"," [-4 -4 -4 -4 -4 -4 -4 -4]\n"," [-4 -3 -3 -3 -3 -3 -3 -3]\n"," [-4 -3 -3 -3 -3 -3 -3 -3]\n"," [-4 -3 -3 -2 -2 -2 -2 -2]\n"," [-4 -3 -3 -2 -1 -1 -1 -2]\n"," [-4 -3 -3 -2 -1  0 -1 -2]]\n","\n"," Improving policy:\n","policy diff: False\n","\n","\n","______iteration: 2 ______\n","\n"," policy:\n","[['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↘', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['→', '↘', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↗', '→', '↘', '↘', '↘', '↘', '↘', '↓'],\n"," ['↗', '↗', '→', '↘', '↘', '↘', '↓', '↙'],\n"," ['↗', '↗', '↗', '→', '↘', '↓', '↙', '↙'],\n"," ['↗', '↗', '↗', '↗', '→', 'F', '←', '←']]\n","\n","Value function for this policy:\n","[[-5 -5 -5 -5 -5 -5 -5 -5]\n"," [-5 -5 -5 -5 -5 -5 -5 -5]\n"," [-4 -4 -4 -4 -4 -4 -4 -4]\n"," [-4 -3 -3 -3 -3 -3 -3 -3]\n"," [-4 -3 -3 -3 -3 -3 -3 -3]\n"," [-4 -3 -3 -2 -2 -2 -2 -2]\n"," [-4 -3 -3 -2 -1 -1 -1 -2]\n"," [-4 -3 -3 -2 -1  0 -1 -2]]\n","\n"," Improving policy:\n","policy diff: True\n","Optimal policy found in 2 steps of policy evaluation\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9zAclY-vPBpE"},"source":["# 1.5 Asynchronous Policy Iteration\n","\n","With policy evaluation, we bootstrap: we make an estimate based on another estimate. So which estimate do we take? We have to options:\n","\n","We bootstrap from the previous policy evaluation. This means each state value estimate update is based on the same iteration of policy evaluation. This is called synchronous policy iteration\n","\n","We bootstrap from the freshest estimate. This means a estimate update can be based on the previous or the current value funtion, or a combination of the two. This is called asynchronous policy iteration"]},{"cell_type":"markdown","metadata":{"id":"1xB87tpRPkjn"},"source":["# 1.6 Value Iteration\n","Theory\n","Value iteration is nothing more than a simple parameter modification to policy iteration. Remember that policy iteration consists of policy evaluation and policy improvement. The policy evaluation step does not necessarily have to be repeated until convergence before we improve our policy. Recall that the policy iteration above took over 400 iterations to converge. If we use ony 1 iteration instead we call it value iteration."]}]}