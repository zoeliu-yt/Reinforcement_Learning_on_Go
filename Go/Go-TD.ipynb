{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./GymGo-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/yuetongliu/Desktop/Reinforcement_Learning_on_Go/Code/14_02_24_Go/GymGo-master\n",
      "Requirement already satisfied: gym in /opt/anaconda3/lib/python3.8/site-packages (from gym-go==0.0.1) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.19.2)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (7.2.0)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-go==0.0.1) (0.18.2)\n",
      "Installing collected packages: gym-go\n",
      "  Attempting uninstall: gym-go\n",
      "    Found existing installation: gym-go 0.0.1\n",
      "    Uninstalling gym-go-0.0.1:\n",
      "      Successfully uninstalled gym-go-0.0.1\n",
      "  Running setup.py develop for gym-go\n",
      "Successfully installed gym-go\n"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program\n",
    "class Learn(object):\n",
    "\n",
    "    # board size, value function, action function\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        # define state value function as a dict\n",
    "        # key is position of b and w\n",
    "        # value is state value (a number)\n",
    "        self.value_function = dict()\n",
    "        \n",
    "        # add new state if not exist\n",
    "        #if tuple(go_env.state()[0:2].reshape(size**2*2)) not in value_function: \n",
    "        #    value_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = 0\n",
    "        \n",
    "        # define action value function as a dict\n",
    "        # key is position of b and w\n",
    "        # value is action value (an array)\n",
    "        self.action_function = dict()\n",
    "        \n",
    "        #if tuple(go_env.state()[0:2].reshape(size**2*2)) not in action_function:\n",
    "        #    action_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = np.zeros(size**2+1)\n",
    "        \n",
    "        self.env = gym.make('gym_go:go-v0', size=self.size, reward_method = \"heuristic\")\n",
    "        \n",
    "        #self.state = self.env.state()[0:2].reshape(size**2*2)\n",
    "    \n",
    "    # helper\n",
    "    def intersection(self,lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "\n",
    "    ## greedy policy\n",
    "    def apply_policy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Apply the policy of the agent\n",
    "        Args:\n",
    "            state: tuple of length 2\n",
    "            epsilon: exploration probability, 0 for greedy behavior, 1 for pure exploration\n",
    "        Returns:\n",
    "            the selected action for the state under the current policy\n",
    "        \"\"\"\n",
    "        ## random\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action_index = self.env.uniform_random_action()\n",
    "        else: \n",
    "            # if not in action function, build a new one\n",
    "            if tuple(state) not in self.action_function:\n",
    "                self.action_function[tuple(state)] = np.zeros(size**2+1)\n",
    "            \n",
    "        \n",
    "            # actions with max value\n",
    "            greedy_action_value = np.max(self.action_function[tuple(state)])\n",
    "            greedy_indices = [i for i, a in enumerate(self.action_function[tuple(state)]) if\n",
    "                          a == greedy_action_value]\n",
    "            \n",
    "            valid = np.nonzero(self.env.valid_moves())[0]\n",
    "            \n",
    "            # overlap between max and valid action \n",
    "            idx = self.intersection(greedy_indices, valid)\n",
    "            if idx == []:\n",
    "                action_index = size**2\n",
    "            #print(greedy_indices)\n",
    "            else:\n",
    "                action_index = np.random.choice(self.intersection(greedy_indices, valid))\n",
    "        \n",
    "        \n",
    "        return action_index\n",
    "    \n",
    "    def sarsa_td(self, n_episodes=1000, alpha=0.01, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Run the sarsa control algorithm (TD0), finding the optimal policy and action function\n",
    "        :param n_episodes: int, amount of episodes to train\n",
    "        :param alpha: learning rate\n",
    "        :param gamma: discount factor of future rewards\n",
    "        :return: finds the optimal policy for move chess\n",
    "        \"\"\"\n",
    "        \n",
    "        for k in range(n_episodes):\n",
    "            self.env = gym.make('gym_go:go-v0', size=self.size, reward_method = \"heuristic\")\n",
    "            \n",
    "            done = False\n",
    "            epsilon = max(1 / (1 + k), 0.05)\n",
    "            \n",
    "            while not done:\n",
    "                # current state - make action\n",
    "                state = self.env.state()[0:2].reshape(size**2*2)\n",
    "                action_index = self.apply_policy(state,epsilon)\n",
    "                \n",
    "                successor_state, reward, done, info = self.env.step(action_index)\n",
    "                successor_state = successor_state[0:2].reshape(size**2*2)\n",
    "                \n",
    "                # successor state - make action\n",
    "                if tuple(successor_state) not in self.action_function:\n",
    "                    self.action_function[tuple(successor_state)] = np.zeros(size**2+1)\n",
    "                \n",
    "                # current state= get action value\n",
    "                if tuple(state) not in self.action_function:\n",
    "                    self.action_function[tuple(state)] = np.zeros(size**2+1)\n",
    "                    action_value = 0\n",
    "                else:\n",
    "                    action_value = self.action_function[tuple(state)][action_index]\n",
    "                    \n",
    "                #successor state - get action value\n",
    "                successor_action_index = self.apply_policy(successor_state, epsilon)\n",
    "\n",
    "                successor_action_value = self.action_function[tuple(successor_state)][successor_action_index]\n",
    "                    \n",
    "                \n",
    "\n",
    "                # update action value function\n",
    "                q_update = alpha * (reward + gamma * successor_action_value - action_value)\n",
    "                self.action_function[tuple(state)][action_index] += q_update\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # white\n",
    "                action = self.env.uniform_random_action()\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "a = Learn(size = 3)\n",
    "a.sarsa_td(n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after traning \n",
    "rewards = []\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "        a.env = gym.make('gym_go:go-v0', size=size, reward_method = 'real')\n",
    "    \n",
    "        \n",
    "            \n",
    "        # number of stone\n",
    "        # number of black and white\n",
    "        # difference\n",
    "        \n",
    "        \n",
    "        \n",
    "        done = False\n",
    "          \n",
    "        while not done:\n",
    "            state = a.env.state()[0:2].reshape(size**2*2)\n",
    "            action_index = a.apply_policy(state, 0.1)\n",
    "            #action = go_env.uniform_random_action()\n",
    "            state, reward, done, info = a.env.step(action_index)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "\n",
    "        # white    \n",
    "            action = a.env.uniform_random_action()\n",
    "            state, reward, done, info = a.env.step(action)\n",
    "            \n",
    "        \n",
    "        rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(609, 49, 342)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.count(1.0),rewards.count(0),rewards.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training \n",
    "rewards1 = []\n",
    "for i in range(1000): \n",
    "    \n",
    "        a.env = gym.make('gym_go:go-v0', size=size, reward_method = 'real')\n",
    "            \n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "             \n",
    "            action = a.env.uniform_random_action()\n",
    "            state, reward, done, info = a.env.step(action)\n",
    "            \n",
    "        \n",
    "        rewards1.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 109, 371)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards1.count(1.0),rewards1.count(0),rewards1.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: reduce state by mirroring, hyperparameter tuning, draw value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 same symmetries\n",
    "def all_symmetries(image):\n",
    "    \"\"\"\n",
    "    :param image: A (C, BOARD_SIZE, BOARD_SIZE) numpy array, where C is any number\n",
    "    :return: All 8 orientations that are symmetrical in a Go game over the 2nd and 3rd axes\n",
    "    (i.e. rotations, flipping and combos of them)\n",
    "    \"\"\"\n",
    "    symmetries = []\n",
    "\n",
    "    for i in range(8):\n",
    "        x = image\n",
    "        if (i >> 0) % 2:\n",
    "            # Horizontal flip\n",
    "            x = np.flip(x, 2)\n",
    "        if (i >> 1) % 2:\n",
    "            # Vertical flip\n",
    "            x = np.flip(x, 1)\n",
    "        if (i >> 2) % 2:\n",
    "            # Rotation 90 degrees\n",
    "            x = np.rot90(x, axes=(1, 2))\n",
    "            \n",
    "        symmetries.append(x)\n",
    "\n",
    "    return symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0.],\n",
       "        [ 0., -1.,  1.],\n",
       "        [ 0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.env = gym.make('gym_go:go-v0', size=size, reward_method = 'real')\n",
    "\n",
    "state, reward, done, info = a.env.step(3)\n",
    "state, reward, done, info = a.env.step(4)\n",
    "#state[0]\n",
    "state[0]-state[1]\n",
    "#all_symmetries(state)\n",
    "all_symmetries((state[0]-state[1]).reshape(1,3,3))[1]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
