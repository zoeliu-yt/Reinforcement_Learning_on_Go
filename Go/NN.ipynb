{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./GymGo-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from gym_go import state_utils, govars\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "from scipy.stats import iqr\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.getcwd())\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "from TDRC.DQRC import DQRC\n",
    "#from TDRC.TDRC import TDRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Structure\n",
    "# mini-batch to update several states at the same time\n",
    "# use target network to gain stablility\n",
    "# replay buffer: iid for SGD optimization\n",
    "# 2. Add Symmetry trick \n",
    "# 3. Add Layer\n",
    "# 3. Get Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_REFRESH = 1\n",
    "ALPHA = 0.0009765\n",
    "size = 3\n",
    "sample_size= 4\n",
    "GAMMA = 0.99\n",
    "train_episodes = 100000\n",
    "eval_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as f\n",
    "from TDRC.utils import getBatchColumns\n",
    "\n",
    "class TDRC:\n",
    "    def __init__(self, features, policy_net, target_net, optimizer, params, device=None):\n",
    "        self.features = features\n",
    "        self.params = params\n",
    "        self.device = device\n",
    "        self.policy_net = policy_net\n",
    "        self.target_net = target_net\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # regularization parameter\n",
    "        self.alpha = params['alpha']\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.beta = params['beta']\n",
    "\n",
    "        # secondary weights optimization parameters\n",
    "        self.beta_1 = params.get('beta_1', 0.99)\n",
    "        self.beta_2 = params.get('beta_2', 0.999)\n",
    "        self.eps = params.get('eps', 1e-8)\n",
    "\n",
    "        # learnable parameters for secondary weights\n",
    "        self.h = torch.zeros(features, requires_grad=False).to(device)\n",
    "        # ADAM optimizer parameters for secondary weights\n",
    "        self.v = torch.zeros(features, requires_grad=False).to(device)\n",
    "        self.m = torch.zeros(features, requires_grad=False).to(device)\n",
    "        \n",
    "    def selectAction(self, x):\n",
    "        # take a random action about epsilon percent of the time\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            a = env.uniform_random_action()\n",
    "            return a\n",
    "\n",
    "        # otherwise take a greedy action\n",
    "        q_s, _ = self.policy_net(x)\n",
    "        return q_s.argmax().detach().cpu().numpy()\n",
    "\n",
    "    def updateNetwork(self, samples):\n",
    "        # organize the mini-batch so that we can request \"columns\" from the data\n",
    "        # e.g. we can get all of the actions, or all of the states with a single call\n",
    "        batch = getBatchColumns(samples)\n",
    "\n",
    "        # compute V(s) for each sample in mini-batch\n",
    "        Vs, x = self.policy_net(batch.states)\n",
    "\n",
    "        # by default V(s') = 0 unless the next states are non-terminal\n",
    "        Vsp = torch.zeros(batch.size, device=self.device)\n",
    "\n",
    "        # if we don't have any non-terminal next states, then no need to bootstrap\n",
    "        if batch.nterm_sp.shape[0] > 0:\n",
    "            Vsp, _ = self.target_net(batch.nterm_sp)\n",
    "\n",
    "        # compute the empirical MSBE for this mini-batch and let torch auto-diff to optimize\n",
    "        # don't worry about detaching the bootstrapping term for semi-gradient TD\n",
    "        # the target network handles that\n",
    "        target = batch.rewards + batch.gamma * Vsp.detach()\n",
    "        td_loss = 0.5 * f.mse_loss(target, Vs)\n",
    "\n",
    "        # compute E[\\delta | x] ~= <h, x>\n",
    "        with torch.no_grad():\n",
    "            delta_hat = torch.matmul(x, self.h.t())\n",
    "\n",
    "        # the gradient correction term is gamma * <h, x> * \\nabla_w V(s')\n",
    "        # to compute this gradient, we use pytorch auto-diff\n",
    "        correction_loss = torch.mean(batch.gamma * delta_hat * Vsp)\n",
    "\n",
    "        # make sure we have no gradients left over from previous update\n",
    "        self.optimizer.zero_grad()\n",
    "        self.target_net.zero_grad()\n",
    "\n",
    "        # compute the entire gradient of the network using only the td error\n",
    "        td_loss.backward()\n",
    "\n",
    "        # if we have non-terminal states in the mini-batch\n",
    "        # the compute the correction term using the gradient of the *target network*\n",
    "        if batch.nterm_sp.shape[0] > 0:\n",
    "            correction_loss.backward()\n",
    "\n",
    "        # add the gradients of the target network for the correction term to the gradients for the td error\n",
    "        for (policy_param, target_param) in zip(self.policy_net.parameters(), self.target_net.parameters()):\n",
    "            policy_param.grad.add_(target_param.grad)\n",
    "\n",
    "        # update the *policy network* using the combined gradients\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # update the secondary weights using a *fixed* feature representation generated by the policy network\n",
    "        with torch.no_grad():\n",
    "            delta = target - Vs\n",
    "            dh = (delta - delta_hat) * x - self.beta * self.h\n",
    "\n",
    "            # ADAM optimizer\n",
    "            # keep a separate set of weights for each action here as well\n",
    "            self.v = self.beta_2 * self.v + (1 - self.beta_2) * (dh**2)\n",
    "            self.m = self.beta_1 * self.m + (1 - self.beta_1) * dh\n",
    "\n",
    "            self.h = self.h + self.alpha * self.m / (torch.sqrt(self.v) + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the structure of our neural network\n",
    "# we need to output both the last layer and the second to last layer\n",
    "# TODO: add layer and nodes\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear((size**2)*2, sample_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(sample_size, sample_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(sample_size, size**2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.model(x)\n",
    "        outputs = self.output(features)\n",
    "        return outputs, features\n",
    "\n",
    "# build the target and policy networks\n",
    "# the target net is the same as the policy net, but with the weights occupying different memory\n",
    "\n",
    "policy_network = Network()\n",
    "target_network = copy.deepcopy(policy_network)\n",
    "optimizer = optim.Adam(policy_network.parameters(), lr = ALPHA, betas=(0.9, 0.999))\n",
    "\n",
    "    \n",
    "# construct our TDRC agent\n",
    "agent = DQRC(sample_size, size**2, policy_network, target_network, optimizer, {\n",
    "    'alpha': ALPHA,\n",
    "    'beta': 1.0,\n",
    "    'epsilon': 0.1,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# much much faster than np.random.choice\n",
    "def choice(arr, size=1):\n",
    "    idxs = np.random.permutation(len(arr))\n",
    "    return [arr[i] for i in idxs[:size]]\n",
    "\n",
    "# a very simple circular replay buffer\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.location = 0\n",
    "        self.buffer = []\n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def add(self, args):\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(args)\n",
    "        else:\n",
    "            self.buffer[self.location] = args\n",
    "        self.location = (self.location + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return choice(self.buffer, batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_episodes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-bd86311840e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mrewards_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gym_go:go-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mreward_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"real\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_episodes' is not defined"
     ]
    }
   ],
   "source": [
    "#env = gym.make('gym_go:go-v0', size=size, reward_method = 'real')\n",
    "#env.reset()\n",
    "# ---------------------\n",
    "# Start the experiments\n",
    "# ---------------------\n",
    "buffer = ReplayBuffer(4000)\n",
    "# current env\n",
    "#s = env.state()\n",
    "# current state\n",
    "#x = torch.tensor(np.concatenate((s[0].reshape(size**2),s[1].reshape(size**2)), axis=None), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "rewards = 0\n",
    "step = 0\n",
    "for i in range(train_episodes):\n",
    "    env = gym.make('gym_go:go-v0', size=size, reward_method = \"real\")\n",
    "    env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        s = env.state()\n",
    "        x = torch.tensor(np.concatenate((s[0].reshape(size**2),s[1].reshape(size**2)), axis=None), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        if step % TARGET_REFRESH == 0:\n",
    "            target_network.load_state_dict(policy_network.state_dict())\n",
    "        \n",
    "        action = agent.selectAction(x)\n",
    "        \n",
    "        #q_s, _ = agent.policy_net(x)\n",
    "        #q_s = q_s.detach().cpu().numpy()   \n",
    "        #q_s = [float('-inf') if s[3].reshape(size**2)[i] == 1 else q_s[0][i] for i in range(size**2)]\n",
    "        #action = np.array(q_s).argmax()\n",
    "        \n",
    "        #print(action)\n",
    "        \n",
    "        valid = np.nonzero(env.valid_moves())[0]\n",
    "        \n",
    "        if action not in valid:\n",
    "            action_index = env.uniform_random_action()\n",
    "        elif isinstance(action, int):\n",
    "            action_index = action\n",
    "        else:\n",
    "            action_index = action.item()\n",
    "            \n",
    "        #print(action_index)\n",
    "            \n",
    "        sp, reward, done, info = env.step(action_index)\n",
    "        \n",
    "        \n",
    "        xp = torch.tensor(np.concatenate((s[0].reshape(size**2),s[1].reshape(size**2)), axis=None), dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        if done:\n",
    "            xp = None\n",
    "            \n",
    "        #print(xp)\n",
    "\n",
    "        a = torch.tensor(action, dtype=torch.int64)\n",
    "        r = torch.tensor(reward, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # use a state-based gamma where gamma = 0.99 of all states\n",
    "        # and gamma = 0 for the terminal state\n",
    "        gamma = torch.tensor(0 if done else GAMMA)\n",
    "        buffer.add((x, a, r, xp, gamma))\n",
    "\n",
    "        \n",
    "        # once the buffer has enough samples to start updating\n",
    "        # then start updating\n",
    "        if step > sample_size:\n",
    "            samples = buffer.sample(sample_size)\n",
    "            agent.updateNetwork(samples)\n",
    "        \n",
    "        s = sp\n",
    "        x = xp\n",
    "        \n",
    "\n",
    "    if reward == 1:\n",
    "        rewards += 1\n",
    "        \n",
    "\n",
    "        \n",
    "rewards_nn = 0\n",
    "for i in range(1000):\n",
    "    env = gym.make('gym_go:go-v0', size=size,  reward_method = \"real\")\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.selectAction(x)\n",
    "        valid = np.nonzero(env.valid_moves())[0]\n",
    "\n",
    "        if action not in valid:\n",
    "            action_index = env.uniform_random_action()\n",
    "        elif isinstance(action, int):\n",
    "            action_index = action\n",
    "        else:\n",
    "            action_index = action.item()\n",
    "\n",
    "\n",
    "        state, reward, done, info = td.env.step(action_index)\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        # white    \n",
    "        action = env.uniform_random_action()\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "    #print(reward)\n",
    "    if reward == 1:\n",
    "        rewards_nn += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_nn = 0\n",
    "for i in range(1000):\n",
    "    env = gym.make('gym_go:go-v0', size=size,  reward_method = \"real\")\n",
    "    env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        s = env.state()\n",
    "        x = torch.tensor(np.concatenate((s[0].reshape(size**2),s[1].reshape(size**2)), axis=None), dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "        action = agent.selectAction(x)\n",
    "        valid = np.nonzero(env.valid_moves())[0]\n",
    "\n",
    "        if action not in valid:\n",
    "            action_index = env.uniform_random_action()\n",
    "        elif isinstance(action, int):\n",
    "            action_index = action\n",
    "        else:\n",
    "            action_index = action.item()\n",
    "\n",
    "\n",
    "        state, reward, done, info = env.step(action_index)\n",
    "\n",
    "\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        # white    \n",
    "        action = env.uniform_random_action()\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "    #print(reward)\n",
    "    if reward == 1:\n",
    "        rewards_nn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "519"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TD training\n",
    "\n",
    "start_time = time.time()\n",
    "rewards_td = dict()\n",
    "for j in train_episodes:\n",
    "    td = Learn(size = size)\n",
    "    td.sarsa_td(n_episodes=j)\n",
    "\n",
    "    reward_td = 0\n",
    "    for i in range(eval_episodes): \n",
    "            td.env = gym.make('gym_go:go-v0', size=size,  reward_method = \"real\")\n",
    "            td.env.reset()\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                states = all_state2num(td.env.state(),size)\n",
    "                action_index = td.apply_policy(states, 0.1)\n",
    "                state, reward, done, info = td.env.step(action_index)\n",
    "\n",
    "\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "                # white    \n",
    "                action = td.env.uniform_random_action()\n",
    "                state, reward, done, info = td.env.step(action)\n",
    "                \n",
    "            #print(reward)\n",
    "            if reward == 1:\n",
    "                reward_td = reward_td + 1\n",
    "    rewards_td[j] = reward_td\n",
    "    print(reward_td)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.5)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(episode_reward, weights=np.ones(len(episode_reward)) / len(episode_reward), bins = 18)\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.ylabel('Percentage of Occurrence')\n",
    "plt.xlabel('TD Reward')\n",
    "plt.ylim([0, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.hist(episode_reward_r, weights=np.ones(len(episode_reward_r)) / len(episode_reward_r), bins = 18)\n",
    "plt.gca().yaxis.set_major_formatter(PercentFormatter(1))\n",
    "plt.ylabel('Percentage of Occurrence')\n",
    "plt.xlabel('TD Reward')\n",
    "plt.ylim([0, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
