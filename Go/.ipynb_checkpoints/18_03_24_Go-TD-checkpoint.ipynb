{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./GymGo-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/yuetongliu/Desktop/Reinforcement_Learning_on_Go/Code/14_02_24_Go/GymGo-master\n",
      "Requirement already satisfied: gym in /opt/anaconda3/lib/python3.8/site-packages (from gym-go==0.0.1) (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (7.2.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.19.2)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-go==0.0.1) (0.18.2)\n",
      "Installing collected packages: gym-go\n",
      "  Attempting uninstall: gym-go\n",
      "    Found existing installation: gym-go 0.0.1\n",
      "    Uninstalling gym-go-0.0.1:\n",
      "      Successfully uninstalled gym-go-0.0.1\n",
      "  Running setup.py develop for gym-go\n",
      "Successfully installed gym-go\n"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 9\n",
    "go_env = gym.make('gym_go:go-v0', size=size, reward_method = \"heuristic\")\n",
    "ept = go_env.state()[0]\n",
    "\n",
    "# define state value function as a dict\n",
    "# key is position of b and w\n",
    "# value is state value (a number)\n",
    "value_function = dict()\n",
    "\n",
    "# add new state if not exist\n",
    "if tuple(go_env.state()[0:2].reshape(size**2*2)) not in value_function:\n",
    "    value_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = 0\n",
    "\n",
    "\n",
    "# define action value function as a dict\n",
    "# key is position of b and w\n",
    "# value is action value (an array)\n",
    "action_function = dict()\n",
    "if tuple(go_env.state()[0:2].reshape(size**2*2)) not in action_function:\n",
    "    action_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = np.zeros(size**2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_policy(state, epsilon):\n",
    "        \"\"\"\n",
    "        Apply the policy of the agent\n",
    "        Args:\n",
    "            state: tuple of length 2\n",
    "            epsilon: exploration probability, 0 for greedy behavior, 1 for pure exploration\n",
    "        Returns:\n",
    "            the selected action for the state under the current policy\n",
    "        \"\"\"\n",
    "        greedy_action_value = np.max(action_function[tuple(state[0:2].reshape(size**2*2))])\n",
    "        greedy_indices = [i for i, a in enumerate(action_function[tuple(state[0:2].reshape(size**2*2))]) if\n",
    "                          a == greedy_action_value]\n",
    "        action_index = np.random.choice(greedy_indices)\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action_index = go_env.uniform_random_action()\n",
    "        return action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = go_env.state()\n",
    "apply_policy(state, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa_td(n_episodes=100, alpha=0.01, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Run the sarsa control algorithm (TD0), finding the optimal policy and action function\n",
    "        :param n_episodes: int, amount of episodes to train\n",
    "        :param alpha: learning rate\n",
    "        :param gamma: discount factor of future rewards\n",
    "        :return: finds the optimal policy for move chess\n",
    "        \"\"\"\n",
    "        \n",
    "        for k in range(n_episodes):\n",
    "            go_env = gym.make('gym_go:go-v0', size=size, reward_method = \"heuristic\")\n",
    "            state = go_env.state()\n",
    "            \n",
    "            done = False\n",
    "            epsilon = max(1 / (1 + k), 0.05)\n",
    "            \n",
    "            # \n",
    "            if tuple(go_env.state()[0:2].reshape(size**2*2)) not in action_function:\n",
    "                action_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = np.zeros(size**2+1)\n",
    "            \n",
    "            while not done:\n",
    "                # make action\n",
    "                # state = self.env.state\n",
    "                action_index = apply_policy(state, epsilon)\n",
    "                \n",
    "                successor_state, reward, done, info = go_env.step(action_index)\n",
    "                #reward, episode_end = self.env.step(action)\n",
    "                \n",
    "                # action value\n",
    "                if tuple(state[0:2].reshape(size**2*2)) not in action_function:\n",
    "                    action_function[tuple(state[0:2].reshape(size**2*2))] = np.zeros(size**2+1)\n",
    "                    action_value = 0\n",
    "                else:\n",
    "                    action_value = action_function[tuple(state[0:2].reshape(size**2*2))][action_index]\n",
    "                \n",
    "                # update action value function\n",
    "                # success \n",
    "                if tuple(successor_state[0:2].reshape(size**2*2)) not in action_function:\n",
    "                    action_function[tuple(successor_state[0:2].reshape(size**2*2))] = np.zeros(size**2+1)\n",
    "                    \n",
    "                #successor_state = self.env.state\n",
    "                successor_action_index = apply_policy(successor_state, epsilon)\n",
    "\n",
    "                successor_action_value = action_function[tuple(state[0:2].reshape(size**2*2))][successor_action_index]\n",
    "                    \n",
    "                \n",
    "\n",
    "                q_update = alpha * (reward + gamma * successor_action_value - action_value)\n",
    "\n",
    "            \n",
    "                action_function[tuple(state[0:2].reshape(size**2*2))][action_index] += q_update\n",
    "                #self.agent.policy = self.agent.action_function.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-b2a87697728c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msarsa_td\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-ce384f2af3f8>\u001b[0m in \u001b[0;36msarsa_td\u001b[0;34m(n_episodes, alpha, gamma)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# update action value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m#successor_state = self.env.state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0msuccessor_action_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccessor_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;31m# action value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-64c38f03dc5c>\u001b[0m in \u001b[0;36mapply_policy\u001b[0;34m(state, epsilon)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mthe\u001b[0m \u001b[0mselected\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mstate\u001b[0m \u001b[0munder\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \"\"\"\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mgreedy_action_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         greedy_indices = [i for i, a in enumerate(action_function[tuple(state[0:2].reshape(size**2*2))]) if\n\u001b[1;32m     12\u001b[0m                           a == greedy_action_value]\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)"
     ]
    }
   ],
   "source": [
    "sarsa_td()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 eposides\n",
    "alpha=0.05\n",
    "lamb=0.9\n",
    "for i in range(10000): \n",
    "    \n",
    "        go_env = gym.make('gym_go:go-v0', size=size, reward_method = \"heuristic\")\n",
    "    \n",
    "        done = False\n",
    "        reward = 0\n",
    "        for i in range(-size**2,size**2+1):\n",
    "            value_function[i].append(0)\n",
    "            \n",
    "        while not done:\n",
    "            action = go_env.uniform_random_action()\n",
    "            suc_state, suc_reward, done, info = go_env.step(action)\n",
    "\n",
    "            value_function[reward].append(value_function[reward][-1] + alpha * (\n",
    "                suc_reward + lamb * value_function[suc_reward][-1] - value_function[reward][-1])) \n",
    "\n",
    "            reward = suc_reward\n",
    "            if go_env.game_ended():\n",
    "                break\n",
    "            \n",
    "\n",
    "        # white    \n",
    "            action = go_env.uniform_random_action()\n",
    "            state_w, reward_w, done, info = go_env.step(action)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
