{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./GymGo-master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/yuetongliu/Desktop/Reinforcement_Learning_on_Go/Code/14_02_24_Go/GymGo-master\n",
      "Requirement already satisfied: gym in /opt/anaconda3/lib/python3.8/site-packages (from gym-go==0.0.1) (0.18.0)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (7.2.0)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/anaconda3/lib/python3.8/site-packages (from gym->gym-go==0.0.1) (1.19.2)\n",
      "Requirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym->gym-go==0.0.1) (0.18.2)\n",
      "Installing collected packages: gym-go\n",
      "  Attempting uninstall: gym-go\n",
      "    Found existing installation: gym-go 0.0.1\n",
      "    Uninstalling gym-go-0.0.1:\n",
      "      Successfully uninstalled gym-go-0.0.1\n",
      "  Running setup.py develop for gym-go\n",
      "Successfully installed gym-go\n"
     ]
    }
   ],
   "source": [
    "! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 same symmetries\n",
    "def all_symmetries(image):\n",
    "    \"\"\"\n",
    "    :param image: A (C, BOARD_SIZE, BOARD_SIZE) numpy array, where C is any number\n",
    "    :return: All 8 orientations that are symmetrical in a Go game over the 2nd and 3rd axes\n",
    "    (i.e. rotations, flipping and combos of them)\n",
    "    \"\"\"\n",
    "    symmetries = []\n",
    "\n",
    "    for i in range(8):\n",
    "        x = image\n",
    "        if (i >> 0) % 2:\n",
    "            # Horizontal flip\n",
    "            x = np.flip(x, 2)\n",
    "        if (i >> 1) % 2:\n",
    "            # Vertical flip\n",
    "            x = np.flip(x, 1)\n",
    "        if (i >> 2) % 2:\n",
    "            # Rotation 90 degrees\n",
    "            x = np.rot90(x, axes=(1, 2))\n",
    "            \n",
    "        symmetries.append(x)\n",
    "\n",
    "    return symmetries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-20bfe2936b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstate2num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# all states to num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mall_state2num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state1' is not defined"
     ]
    }
   ],
   "source": [
    "size = 5\n",
    "# one state to num\n",
    "def state2num(state1):\n",
    "    # state1 is a size*size*6 matrix\n",
    "    \n",
    "    \n",
    "    state1 = state1.astype(int)\n",
    "    state = state1[0:2].reshape(size**2*2)\n",
    "    num = int(\"\".join(str(x) for x in state), 2)\n",
    "    \n",
    "    return num\n",
    "state2num(state1)\n",
    "# all states to num\n",
    "def all_state2num(state1):\n",
    "    # state1 is a size*size*6 matrix\n",
    "    state1 = state1.astype(int)\n",
    "    all_b = all_symmetries(state1[0].reshape(1,size,size))\n",
    "    all_w = all_symmetries(state1[1].reshape(1,size,size))\n",
    "    ls = []\n",
    "    for i in range(8):\n",
    "        state = np.concatenate((all_b[i].reshape(size**2),all_w[i].reshape(size**2)), axis=None)\n",
    "        num = int(\"\".join(str(x) for x in state), 2)\n",
    "        ls.append(num)\n",
    "    return ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_state2num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c77c131fbb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_state2num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m70368745226240\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_state2num' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "ls = all_state2num(state1)\n",
    "l = dict()\n",
    "l[70368745226240] = a\n",
    "\n",
    "action = 16\n",
    "def symmetry_action(ls, l, action):\n",
    "    # ls is all symmetry fucntion\n",
    "    # l is all keys in value function\n",
    "    # action is an int\n",
    "    \n",
    "    # return rotated action and flag indicating if the state exists\n",
    "    flag = False\n",
    "    for i in range(len(ls)):\n",
    "        if ls[i] in l:\n",
    "            flag = True\n",
    "            \n",
    "            a = math.floor(action/size)\n",
    "            b = action%size\n",
    "            s = size - 1\n",
    "            \n",
    "            if i == 2:\n",
    "                b = s-b\n",
    "            elif i == 3:\n",
    "                a = s-a\n",
    "            elif i == 4:\n",
    "                a = s-a\n",
    "                b = s-b\n",
    "            elif i == 5:\n",
    "                b = a\n",
    "                a = b\n",
    "            elif i == 6:\n",
    "                a = s-b\n",
    "                b = a\n",
    "            elif i == 7:\n",
    "                a = b\n",
    "                b = s-a\n",
    "            elif i == 8:\n",
    "                a = s-b\n",
    "                b = s-a\n",
    "            action = size * a + b\n",
    "\n",
    "            break\n",
    "    return action, flag\n",
    "[action, flag] = symmetry_action(ls, l, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program\n",
    "class Learn(object):\n",
    "\n",
    "    # board size, value function, action function\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        # define state value function as a dict\n",
    "        # key is position of b and w\n",
    "        # value is state value (a number)\n",
    "        self.value_function = dict()\n",
    "        \n",
    "        # add new state if not exist\n",
    "        #if tuple(go_env.state()[0:2].reshape(size**2*2)) not in value_function: \n",
    "        #    value_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = 0\n",
    "        \n",
    "        # define action value function as a dict\n",
    "        # key is position of b and w\n",
    "        # value is action value (an array)\n",
    "        self.action_function = dict()\n",
    "        \n",
    "        #if tuple(go_env.state()[0:2].reshape(size**2*2)) not in action_function:\n",
    "        #    action_function[tuple(go_env.state()[0:2].reshape(size**2*2))] = np.zeros(size**2+1)\n",
    "        \n",
    "        self.env = gym.make('gym_go:go-v0', size=self.size, reward_method = \"heuristic\")\n",
    "        \n",
    "        #self.state = self.env.state()[0:2].reshape(size**2*2)\n",
    "    \n",
    "    # helper\n",
    "    def intersection(self,lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3\n",
    "\n",
    "    ## greedy policy\n",
    "    def apply_policy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Apply the policy of the agent\n",
    "        Args:\n",
    "            state: tuple of length 2\n",
    "            epsilon: exploration probability, 0 for greedy behavior, 1 for pure exploration\n",
    "        Returns:\n",
    "            the selected action for the state under the current policy\n",
    "        \"\"\"\n",
    "        ## random\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action_index = self.env.uniform_random_action()\n",
    "        else: \n",
    "            # if not in action function, build a new one\n",
    "            if tuple(state) not in self.action_function:\n",
    "                self.action_function[tuple(state)] = np.zeros(size**2+1)\n",
    "            \n",
    "        \n",
    "            # actions with max value\n",
    "            greedy_action_value = np.max(self.action_function[tuple(state)])\n",
    "            greedy_indices = [i for i, a in enumerate(self.action_function[tuple(state)]) if\n",
    "                          a == greedy_action_value]\n",
    "            \n",
    "            valid = np.nonzero(self.env.valid_moves())[0]\n",
    "            \n",
    "            # overlap between max and valid action \n",
    "            idx = self.intersection(greedy_indices, valid)\n",
    "            if idx == []:\n",
    "                action_index = size**2\n",
    "            #print(greedy_indices)\n",
    "            else:\n",
    "                action_index = np.random.choice(self.intersection(greedy_indices, valid))\n",
    "        \n",
    "        \n",
    "        return action_index\n",
    "    \n",
    "    def sarsa_td(self, n_episodes=1000, alpha=0.01, gamma=0.9):\n",
    "        \"\"\"\n",
    "        Run the sarsa control algorithm (TD0), finding the optimal policy and action function\n",
    "        :param n_episodes: int, amount of episodes to train\n",
    "        :param alpha: learning rate\n",
    "        :param gamma: discount factor of future rewards\n",
    "        :return: finds the optimal policy for move chess\n",
    "        \"\"\"\n",
    "        \n",
    "        for k in range(n_episodes):\n",
    "            self.env = gym.make('gym_go:go-v0', size=self.size, reward_method = \"heuristic\")\n",
    "            \n",
    "            done = False\n",
    "            epsilon = max(1 / (1 + k), 0.05)\n",
    "            \n",
    "            while not done:\n",
    "                # current state - action idx\n",
    "                state = self.env.state()[0:2].reshape(size**2*2)\n",
    "                states = all_state2num(self.env.state())\n",
    "                action_index = self.apply_policy(state,epsilon)\n",
    "                \n",
    "                \n",
    "                # current state - action value\n",
    "                ## check all rotations\n",
    "                [action_idx, flag] = symmetry_action(states, self.action_function, action_index)\n",
    "                if not flag:\n",
    "                    self.action_function[tuple(state)] = np.zeros(size**2+1)\n",
    "                \n",
    "                action_value = self.action_function[tuple(state)][action_index]\n",
    "                \n",
    "                \n",
    "                # step\n",
    "                successor_state, reward, done, info = self.env.step(action_index)\n",
    "                \n",
    "                \n",
    "                # successor state - action idx\n",
    "                successor_state = successor_state[0:2].reshape(size**2*2)\n",
    "                successor_states = all_state2num(successor_state)\n",
    "                successor_action_index = self.apply_policy(successor_state, epsilon)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # successor state - action value\n",
    "                if tuple(successor_state) not in self.action_function:\n",
    "                    self.action_function[tuple(successor_state)] = np.zeros(size**2+1)\n",
    "                    \n",
    "                successor_action_value = self.action_function[tuple(successor_state)][successor_action_index]\n",
    "                \n",
    "                # update action value function\n",
    "                q_update = alpha * (reward + gamma * successor_action_value - action_value)\n",
    "                self.action_function[tuple(state)][action_index] += q_update\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                # white\n",
    "                action = self.env.uniform_random_action()\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "a = Learn(size = 5)\n",
    "#a.sarsa_td(n_episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after traning \n",
    "rewards = []\n",
    "\n",
    "for i in range(1000): \n",
    "    \n",
    "        a.env = gym.make('gym_go:go-v0', size=size, reward_method = 'real')\n",
    "    \n",
    "        \n",
    "            \n",
    "        # number of stone\n",
    "        # number of black and white\n",
    "        # difference\n",
    "        \n",
    "        \n",
    "        \n",
    "        done = False\n",
    "          \n",
    "        while not done:\n",
    "            state = a.env.state()[0:2].reshape(size**2*2)\n",
    "            action_index = a.apply_policy(state, 0.1)\n",
    "            #action = go_env.uniform_random_action()\n",
    "            state, reward, done, info = a.env.step(action_index)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "\n",
    "        # white    \n",
    "            action = a.env.uniform_random_action()\n",
    "            state, reward, done, info = a.env.step(action)\n",
    "            \n",
    "        \n",
    "        rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(609, 49, 342)"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards.count(1.0),rewards.count(0),rewards.count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training \n",
    "rewards1 = []\n",
    "for i in range(1000): \n",
    "    \n",
    "        a.env = gym.make('gym_go:go-v0', size=size, reward_method = 'real')\n",
    "            \n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "             \n",
    "            action = a.env.uniform_random_action()\n",
    "            state, reward, done, info = a.env.step(action)\n",
    "            \n",
    "        \n",
    "        rewards1.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 109, 371)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards1.count(1.0),rewards1.count(0),rewards1.count(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
