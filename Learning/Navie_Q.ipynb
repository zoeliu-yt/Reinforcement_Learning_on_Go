{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03135789,  0.02763279, -0.03062558,  0.02469429])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qvals = {a: Q[env.reset(), a] for a in actions}\n",
    "qvals\n",
    "actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warpper\n",
    "\n",
    "Most gym environment have space in multi-dimension (Box format). To save memory, we apply a wrapper to discretize the observation. \n",
    "\n",
    "env.unwrapped will give back the internal original environment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-33f958d033c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m env = DiscretizedObservationWrapper(\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-33f958d033c9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, n_bins, low, high)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_bins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlow\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class DiscretizedObservationWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"This wrapper converts a Box observation into a single integer.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_bins=10, low=None, high=None):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
    "\n",
    "        low = self.observation_space.low if low is None else low\n",
    "        high = self.observation_space.high if high is None else high\n",
    "\n",
    "        self.n_bins = n_bins\n",
    "        self.val_bins = [np.linspace(l, h, n_bins + 1) for l, h in\n",
    "                         zip(low, high)]\n",
    "        self.observation_space = gym.spaces.Discrete(n_bins ** len(low))\n",
    "\n",
    "    def _convert_to_one_number(self, digits):\n",
    "        return sum([d * ((self.n_bins + 1) ** i) for i, d in enumerate(digits)])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        digits = [np.digitize([x], bins)[0]\n",
    "                  for x, bins in zip(observation, self.val_bins)]\n",
    "        return self._convert_to_one_number(digits)\n",
    "\n",
    "\n",
    "env = DiscretizedObservationWrapper(\n",
    "    env, \n",
    "    n_bins=0, \n",
    "    low=[0,0,0,0], \n",
    "    high=[1,1,1,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low=[0,0,0,0], \n",
    "high=[1,1,1,1]\n",
    "val_bins = [np.linspace(l, h, 1 + 1) for l, h in\n",
    "                         zip(low, high)]\n",
    "val_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-cce258da9861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_bins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdigitize\u001b[0;34m(x, bins, right)\u001b[0m\n\u001b[1;32m   4774\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x may not be complex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4776\u001b[0;31m     \u001b[0mmono\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_monotonicity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4777\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmono\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4778\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bins must be monotonically increasing or decreasing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "\n",
    "for x, bins in zip(env.reset(), val_bins):\n",
    "    np.digitize([x], bins)[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "Q = defaultdict(float)\n",
    "\n",
    "gamma = 0.99  # Discounting factor\n",
    "alpha = 0.5  # soft update param\n",
    "actions = range(env.action_space.n) # all possible actions\n",
    "\n",
    "# Calculate Q Value    \n",
    "def update_Q(s, r, a, s_next, done):\n",
    "    # s: observations\n",
    "    # r: reward \n",
    "    # a: actions\n",
    "    # s_next: same as s\n",
    "    # done: bool\n",
    "    \n",
    "    max_q_next = max([Q[s_next, a] for a in actions])  # get max Q value of (s', a')\n",
    "    \n",
    "    # Do not include the next state's value if currently at the terminal state.\n",
    "    Q[s, a] += alpha * (r + gamma * max_q_next * (1.0 - done) - Q[s, a]) # the Bellman equation to update Q value. \n",
    "    # If done is True, Q(s,a) = (1-alpha)*Q(s,a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick Action \n",
    "\n",
    "Action is decided based on max Q value and we use ε-greedy to force exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1  # 10% chances to apply a random action\n",
    "\n",
    "def act(ob): \n",
    "    if np.random.random() < epsilon:\n",
    "        # action_space.sample() is a convenient function to get a random action\n",
    "        # that is compatible with this given action space.\n",
    "        return env.action_space.sample()\n",
    "\n",
    "    # Pick the action with highest q value.\n",
    "    qvals = {a: Q[ob, a] for a in actions}\n",
    "    max_q = max(qvals.values())  # get best Q value \n",
    "    # In case multiple actions have the same maximum q value.\n",
    "    actions_with_max_q = [a for a, q in qvals.items() if q == max_q] # range(2); max_q\n",
    "    return np.random.choice(actions_with_max_q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "Compare the total reward using Naive Q-learning and random play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 10000\n",
    "rewards = []\n",
    "for e in range(NUM_EPISODES):\n",
    "    \n",
    "    ob = env.reset()\n",
    "     # reward for each round \n",
    "    reward = 0.0 # total reward in one round\n",
    "\n",
    "    while True:\n",
    "        a = act(ob)\n",
    "        ob_next, r, done, _ = env.step(a)\n",
    "        update_Q(ob, r, a, ob_next, done)\n",
    "        reward += r\n",
    "        ob = ob_next\n",
    "        \n",
    "        if done:\n",
    "            #print(done)\n",
    "            break\n",
    "    #print(1)     \n",
    "    rewards.append(reward)   \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_r = [] # reward for each round \n",
    "for e in range(NUM_EPISODES):\n",
    "    \n",
    "    ob = env.reset()\n",
    "    \n",
    "    reward = 0.0 # total reward in one round\n",
    "\n",
    "    while True:\n",
    "    #env.render()\n",
    "        ob_next, r, done, _ = env.step(env.action_space.sample())\n",
    "        reward += r\n",
    "        ob = ob_next\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    \n",
    "    rewards_r.append(reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "n_steps = 10000\n",
    "\n",
    "ob = env.reset()\n",
    "rewards_r = [] # reward for each round \n",
    "reward = 0.0 # total reward in one round\n",
    "for step in range(n_steps):\n",
    "    #env.render()\n",
    "    \n",
    "    ob_next, r, done, _ = env.step(env.action_space.sample())\n",
    "    reward += r\n",
    "    if done:\n",
    "        rewards_r.append(reward)\n",
    "        reward = 0.0\n",
    "        ob = env.reset()\n",
    "    else:\n",
    "        ob = ob_next\n",
    "        \n",
    "env.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.boxplot(x=rewards).set_title('reward for naive_q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=rewards_r).set_title('reward for random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(rewards) \n",
    "df.to_csv('Q.csv')  \n",
    "df = pd.DataFrame(rewards_r) \n",
    "df.to_csv('Random.csv') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, there is a significant increase on reward after applying Naive Q_learning method. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
