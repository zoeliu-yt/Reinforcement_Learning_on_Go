{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyvirtualdisplay\n",
      "  Downloading PyVirtualDisplay-2.1-py3-none-any.whl (15 kB)\n",
      "Collecting EasyProcess\n",
      "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
      "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
      "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf_agents\n",
      "  Downloading tf_agents-0.7.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.11.3 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (3.14.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (1.12.1)\n",
      "Requirement already satisfied: gym>=0.17.0 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (0.18.0)\n",
      "Collecting tensorflow-probability>=0.12.1\n",
      "  Downloading tensorflow_probability-0.12.2-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.8 MB 12.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=7.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (7.2.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (1.19.2)\n",
      "Collecting gin-config>=0.4.0\n",
      "  Downloading gin_config-0.4.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 46 kB 6.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from tf_agents) (3.7.4.3)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (from gym>=0.17.0->tf_agents) (1.5.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym>=0.17.0->tf_agents) (1.5.0)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow-probability>=0.12.1->tf_agents) (4.4.2)\n",
      "Requirement already satisfied: gast>=0.3.2 in /opt/anaconda3/lib/python3.8/site-packages (from tensorflow-probability>=0.12.1->tf_agents) (0.3.3)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.6-cp38-cp38-macosx_10_14_x86_64.whl (95 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95 kB 7.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in /opt/anaconda3/lib/python3.8/site-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf_agents) (0.18.2)\n",
      "Installing collected packages: dm-tree, tensorflow-probability, gin-config, tf-agents\n",
      "Successfully installed dm-tree-0.1.6 gin-config-0.4.0 tensorflow-probability-0.12.2 tf-agents-0.7.1\n"
     ]
    }
   ],
   "source": [
    "! pip install tf_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyvirtualdisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "import PIL.Image\n",
    "from tf_agents.policies import random_tf_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_v2_behavior()\n",
    "# Set up a virtual display for rendering OpenAI gym environments.\n",
    "#display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "log_interval = 200  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = suite_gym.load(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluation(Testing)\n",
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "# Python to TF\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent / Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y(s,a,r,s‚Ä≤)=r+Œ≥maxa‚Ä≤QŒ∏‚àí(s‚Ä≤,a‚Ä≤)\n",
    "\n",
    "L(Œ∏)=ùîº(s,a,r,s‚Ä≤)‚àºU(D)[(Y(s,a,r,s‚Ä≤)‚àíQŒ∏(s,a))2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a QNetwork\n",
    "Feed Forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_layer_params = (100,) # initial_collect_steps? input layer?\n",
    "\n",
    "q_net = q_network.QNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    # A list of fully_connected parameters, \n",
    "    # where each item is the number of units in the layer\n",
    "    fc_layer_params=fc_layer_params) \n",
    "#q_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a DqnAgent \n",
    "\n",
    "Implements the DQN algorithm from\n",
    "\n",
    "\"Human level control through deep reinforcement learning\" Mnih et al., 2015\n",
    "\n",
    "https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) # Adam algorithm\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "agent = dqn_agent.DqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    q_network=q_net,\n",
    "    optimizer=optimizer, # AdamOptimizer function\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss, # loss function\n",
    "    train_step_counter=train_step_counter) # integer step counter\n",
    "\n",
    "agent.initialize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies/Rules\n",
    "\n",
    "The rules will return an action to produce the desired rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = agent.policy # The main policy that is used for evaluation and deployment.\n",
    "collect_policy = agent.collect_policy # A second policy that is used for data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Computes the average return of a policy per episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show baseline performance by randomly selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.9"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "Dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), observation=BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
       "       dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
       "       dtype=float32)), action=BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)), policy_info=(), next_step_type=TensorSpec(shape=(), dtype=tf.int32, name='step_type'), reward=TensorSpec(shape=(), dtype=tf.float32, name='reward'), discount=BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))),\n",
       " ('step_type',\n",
       "  'observation',\n",
       "  'action',\n",
       "  'policy_info',\n",
       "  'next_step_type',\n",
       "  'reward',\n",
       "  'discount'))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    # The maximum number of items that can be stored in a \n",
    "    # single batch segment of the buffer\n",
    "    max_length=replay_buffer_max_length) \n",
    "\n",
    "agent.collect_data_spec, agent.collect_data_spec._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recording the data in the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(traj)\n",
    "\n",
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert the dictionary as a data set and return an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, \n",
    "    sample_batch_size=batch_size, \n",
    "    num_steps=2).prefetch(3)\n",
    "#dataset\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "Two things must happen during the training loop:\n",
    "\n",
    "collect data from the environment\n",
    "\n",
    "use that data to train the agent's neural network(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 200: loss = 18.398178100585938\n",
      "step = 400: loss = 14.568204879760742\n",
      "step = 600: loss = 3.974918842315674\n",
      "step = 800: loss = 7.82936954498291\n",
      "step = 1000: loss = 2.6454858779907227\n",
      "step = 1200: loss = 6.08250617980957\n",
      "step = 1400: loss = 16.339977264404297\n",
      "step = 1600: loss = 5.506181716918945\n",
      "step = 1800: loss = 39.102577209472656\n",
      "step = 2000: loss = 28.136919021606445\n",
      "step = 2200: loss = 44.216697692871094\n",
      "step = 2400: loss = 24.06719207763672\n",
      "step = 2600: loss = 10.0909423828125\n",
      "step = 2800: loss = 39.80750274658203\n",
      "step = 3000: loss = 18.409879684448242\n",
      "step = 3200: loss = 21.06759262084961\n",
      "step = 3400: loss = 25.642637252807617\n",
      "step = 3600: loss = 70.43470764160156\n",
      "step = 3800: loss = 12.162662506103516\n",
      "step = 4000: loss = 7.119634628295898\n",
      "step = 4200: loss = 14.154354095458984\n",
      "step = 4400: loss = 49.889259338378906\n",
      "step = 4600: loss = 29.47924041748047\n",
      "step = 4800: loss = 30.541362762451172\n",
      "step = 5000: loss = 90.28516387939453\n",
      "step = 5200: loss = 63.75636291503906\n",
      "step = 5400: loss = 19.48150634765625\n",
      "step = 5600: loss = 24.87364387512207\n",
      "step = 5800: loss = 63.14601516723633\n",
      "step = 6000: loss = 205.32388305664062\n",
      "step = 6200: loss = 54.176429748535156\n",
      "step = 6400: loss = 85.33269500732422\n",
      "step = 6600: loss = 218.08676147460938\n",
      "step = 6800: loss = 47.16614532470703\n",
      "step = 7000: loss = 70.4386215209961\n",
      "step = 7200: loss = 27.960838317871094\n",
      "step = 7400: loss = 59.63410568237305\n",
      "step = 7600: loss = 7.776348114013672\n",
      "step = 7800: loss = 14.587772369384766\n",
      "step = 8000: loss = 7.049830436706543\n",
      "step = 8200: loss = 168.56646728515625\n",
      "step = 8400: loss = 58.932586669921875\n",
      "step = 8600: loss = 9.754995346069336\n",
      "step = 8800: loss = 130.88519287109375\n",
      "step = 9000: loss = 80.58397674560547\n",
      "step = 9200: loss = 138.56503295898438\n",
      "step = 9400: loss = 27.045242309570312\n",
      "step = 9600: loss = 115.84988403320312\n",
      "step = 9800: loss = 241.87728881835938\n",
      "step = 10000: loss = 156.74887084960938\n",
      "step = 10200: loss = 118.97740173339844\n",
      "step = 10400: loss = 34.707801818847656\n",
      "step = 10600: loss = 487.5429382324219\n",
      "step = 10800: loss = 399.7189636230469\n",
      "step = 11000: loss = 423.0207214355469\n",
      "step = 11200: loss = 35.36727523803711\n",
      "step = 11400: loss = 2392.07861328125\n",
      "step = 11600: loss = 283.6932678222656\n",
      "step = 11800: loss = 143.21328735351562\n",
      "step = 12000: loss = 422.8373107910156\n",
      "step = 12200: loss = 2575.525634765625\n",
      "step = 12400: loss = 22.198543548583984\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f188ebe1ba08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;31m#if step % eval_interval == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mavg_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_avg_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;31m#print('step = {0}: Average Return = {1}'.format(step, avg_return))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-db5feda33c12>\u001b[0m in \u001b[0;36mcompute_avg_return\u001b[0;34m(environment, policy, num_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0maction_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mtime_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mepisode_return\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/utils/common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[0;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[0;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \"\"\"\n\u001b[1;32m    559\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tf_agents_tf_policy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0mdistribution_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-arg-types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     actions = tf.nest.map_structure(\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/policies/greedy_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mDeterministicWithLogProb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     distribution_step = self._wrapped_policy.distribution(\n\u001b[0m\u001b[1;32m     81\u001b[0m         time_step, policy_state)\n\u001b[1;32m     82\u001b[0m     return policy_step.PolicyStep(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/policies/tf_policy.py\u001b[0m in \u001b[0;36mdistribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memit_log_probability\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m       \u001b[0;31m# This here is set only for compatibility with info_spec in constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/policies/q_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[0;34m(self, time_step, policy_state)\u001b[0m\n\u001b[1;32m    151\u001b[0m           network_observation)\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     q_values, policy_state = self._q_network(\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mnetwork_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         step_type=time_step.step_type)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m       \u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"network_state\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnormalized_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     nest_utils.assert_matching_dtypes_and_inner_shapes(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1011\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1012\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/networks/q_network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, observation, step_type, network_state, training)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \"\"\"\n\u001b[0;32m--> 145\u001b[0;31m     state, network_state = self._encoder(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnetwork_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         training=training)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tf_agents/networks/network.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;31m# Convert *args, **kwargs to a canonical kwarg representation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m     normalized_kwargs = tf_inspect.getcallargs(\n\u001b[0m\u001b[1;32m    393\u001b[0m         self.call, inputs, *args, **kwargs)\n\u001b[1;32m    394\u001b[0m     \u001b[0;31m# TODO(b/156315434): Rename network_state to just state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetcallargs\u001b[0;34m(*func_and_positional, **named)\u001b[0m\n\u001b[1;32m    276\u001b[0m   \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_positional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mpositional\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_positional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m   \u001b[0margspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m   \u001b[0mcall_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m   \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'im_self'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__self__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1166\u001b[0m             \u001b[0mvarkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m             \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/inspect.py\u001b[0m in \u001b[0;36mannotation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2536\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "  %%time\n",
    "except:\n",
    "  pass\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = []\n",
    "losses = []\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience).loss\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
    "\n",
    "  #if step % eval_interval == 0:\n",
    "  avg_return = compute_avg_return(eval_env, agent.policy, 1)\n",
    "    #print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
    "  returns.append(avg_return)\n",
    "  losses.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5. ,   4.5,   5. , ..., 100. ,  99.5,  90.5], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(returns)\n",
    "import numpy as np\n",
    "np.array(returns)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterations = range(0, 12550)\n",
    "plt.plot(iterations, np.array(returns)/2)\n",
    "plt.ylabel('Reward-Deep-Q')\n",
    "plt.xlabel('Iterations')\n",
    "#plt.ylim(top=250)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, losses)\n",
    "plt.ylabel('Loss-Deep-Q')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, returns)\n",
    "plt.ylabel('Reward-Deep-Q-Memory')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(iterations, losses)\n",
    "plt.ylabel('Loss-Deep-Q-Memory')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylim(top=250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
